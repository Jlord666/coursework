<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0834)https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&ik=c9b4c84489&view=att&th=15aa715e60d1acc3&attid=0.1.1&disp=inline&safe=1&zw&saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg -->
<html class="gr__mail-attachment_googleusercontent_com"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></head><body data-gr-c-s-loaded="true">




































<div>





<div>











<div>



<h1>Biomath 204: Biomedical Data Analysis</h1>
<h4><em>Dr.&nbsp;Hua Zhou (<a href="http://huazhou@ucla.edu/" target="_blank">huazhou@ucla.edu</a>)</em></h4>
<h4><em>Mar 6/8, 2017</em></h4>

</div>

<div>
<ul>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_challenger-and-o-ring">Challenger and O-ring</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_binomial-regression-model">Binomial regression model</a><ul>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_parameter-estimation-fitted-values-and-prediction">Parameter estimation, fitted values, and prediction</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_inference">Inference</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_odds-odds-ratio-risk">Odds, odds ratio, risk</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_example-infant-respiratory-disease">Example: infant respiratory disease</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_prospective-and-retrospective-sampling">Prospective and retrospective sampling</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_choice-of-link-functions">Choice of link functions</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_confidence-interval-on-prediction">Confidence interval on prediction</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_separation-and-convergence-issues">Separation and convergence issues</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_overdispersion">Overdispersion</a></li>
</ul></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_multinomial-regression">Multinomial regression</a><ul>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_multinomial-logit-model">Multinomial logit model</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_hierarchical-or-nested-responses">Hierarchical or nested responses</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_ordinal-multinomial-responses">Ordinal multinomial responses</a><ul>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_proportional-odds-model">Proportional odds model</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_ordered-probit-model">Ordered probit model</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_proportional-hazards-model">Proportional hazards model</a></li>
</ul></li>
</ul></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_glm">GLM</a><ul>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_exponential-family-distribution">Exponential family distribution</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_link-function">Link function</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_fisher-scoring-algorithm-and-irwls">Fisher scoring algorithm and IRWLS</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_hypothesis-testing">Hypothesis testing</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_diagnostics">Diagnostics</a><ul>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_residuals">Residuals</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_leverage-and-influence">Leverage and influence</a></li>
<li><a href="https://mail-attachment.googleusercontent.com/attachment/u/0/?ui=2&amp;ik=c9b4c84489&amp;view=att&amp;th=15aa715e60d1acc3&amp;attid=0.1.1&amp;disp=inline&amp;safe=1&amp;zw&amp;saddbat=ANGjdJ-W65Y21KSsdgtTt68glvqIZUKQWWU7HI78O4LxpOnG8DUx-XelTbPnZ4Btka7Btsrb3ypxWzpevJs4yLuLvNrNYqnPGmgDoScXzUcYw7bOoulJ-eaRUqwoy-fz-xm9zMYd0THOJtNuJuvdEmxoaeTVKkJzsbFfj-UCCR343Al2TFKi-52apKdTtNABzkpS-PTibuumrdzcT1oZDRZzFugbuosxqG8vZw-LpSc0qS-HuDDYy0CNpg9IdUFD-bxHZzMrEuQ48ovwOuldX_hUmYF3_Jknc40PGUDAdIEEqeifRpRzRpKjWXu1C2ccoT3PX7C6n6Rvbp6L1qv2-lOS3BKQQHZqiasiD-jATxhHTK6N59mWgzbTBDgMfZCE83dWsXVtKj7E6TePdua0BYgHmncqrPhWqK-tUMcR35O_Lzo9ubTtwQj9dYOjSOryDeXp5MUUXW4kONFa-HnIJg6riRIj2lcqI_ZjQ1Aw2NVXN3mFVy0705x8IHjii4aO6rU6AWJSKGbQjdq40q0bsHH-EdGInB2P9G5DBL3sZYCg3YiR2yZK2sHF6eNBM2DXYDByKtMvzegEQ41V6iIi9yFL4EXgSj5sAv3qK3rVhmJ0DBJzxfpInHOruBwDLaWYWrbJSbiRwAHs3I5QyAnRLzE6RH7VrqsIBIk8tAYqLg#0.1.1_residual-plots">Residual plots</a></li>
</ul></li>
</ul></li>
</ul>
</div>

<p>This note is for my guest lectures on <em>Biomath 204: Biomedical Data Analysis</em> taught by Prof.&nbsp;Marc Suchard. It roughly follows Chapters 14 and 15 of <a href="https://us.sagepub.com/en-us/nam/applied-regression-analysis-and-generalized-linear-models/book237254%20#contents" target="_blank"><em>Applied Regression Analysis and Generalized Linear Models</em></a> by John Fox and the book <a href="https://www.amazon.com/Extending-Linear-Model-Generalized-Nonparametric/dp/158488424X" target="_blank"><em>Extending the Linear Model with R</em></a> by Julian Faraway.</p>
<p>System information is displayed for reproducibility.</p>
<pre><code>sessionInfo()</code></pre>
<pre><code>## R version 3.3.2 (2016-10-31)
## Platform: x86_64-apple-darwin13.4.0 (64-bit)
## Running under: macOS Sierra 10.12.3
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.<wbr>UTF-8/C/en_US.UTF-8/en_US.UTF-<wbr>8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## loaded via a namespace (and not attached):
##  [1] backports_1.0.5 magrittr_1.5    rprojroot_1.2   tools_3.3.2    
##  [5] htmltools_0.3.5 yaml_2.1.14     Rcpp_0.12.9     stringi_1.1.2  
##  [9] rmarkdown_1.3   knitr_1.15.1    stringr_1.2.0   digest_0.6.12  
## [13] evaluate_0.10</code></pre>
<div>
<h1>Challenger and O-ring</h1>
<div>
<img>

</div>
<div>
<img>

</div>
<div>
<img>

</div>
<div>
<img>

</div>
<ul>
<li>In January 1986, the space shuttle Challenger exploded 73 seconds after launch.<br>
</li>
<li>The culprit is the O-ring seals in the rocket boosters. At lower temperatures, rubber becomes more brittle and is a less effective sealant. At the time of the launch, the temperature was 31°F.<br>
</li>
<li>Could the failure of the O-rings have been predicted?<br>
</li>
<li>Data: 23 previous shuttle missions. Each shuttle had 2 boosters, each with 3 O-rings. We know the number of O-rings out of 6 showing some damage and the launch temperature.</li>
</ul>
<pre><code>library(faraway) # load the datasets in Faraway book
data(orings)
orings</code></pre>
<pre><code>##    temp damage
## 1    53      5
## 2    57      1
## 3    58      1
## 4    63      1
## 5    66      0
## 6    67      0
## 7    67      0
## 8    67      0
## 9    68      0
## 10   69      0
## 11   70      1
## 12   70      0
## 13   70      1
## 14   70      0
## 15   72      0
## 16   73      0
## 17   75      0
## 18   75      1
## 19   76      0
## 20   76      0
## 21   78      0
## 22   79      0
## 23   81      0</code></pre>
<p>We are interested in how the probability of failure in a given O-ring is related to the launch temperature and predicting that probability when the temperature is 31°F. A naive approach, based on linear models, simply fits a line to this data:</p>
<pre><code>plot(damage / 6 ~ temp, orings, xlim = c(25, 85), ylim = c(0, 1), xlab = "Temperature", ylab = "Prob of damage")
lmod &lt;- lm(damage / 6 ~ temp, orings)
abline(lmod)</code></pre>
<p><img width="672"></p>
<p>Immediately we see one issue. The preditted probabilities of damage can be &gt;1 or &lt;0 at extreme temperatures. This does not make sense.</p>
</div>
<div>
<h1>Binomial regression model</h1>
<p>Let’s assume <span>\(Y_i \sim \text{Bin}(n_i, p_i)\)</span> <span>\[
  \mathbb{P}(Y_i = y_i) = \binom{n_i}{y_i} p_i^{y_i} (1 - p_i)^{n_i - y_i}
\]</span> and are independent. For example in the O-ring data, <span>\(n_i=6\)</span>. To link the mean of <span>\(Y_i\)</span> to covariates <span>\(x_i = (x_{i1}, \ldots, x_{iq})\)</span>, we assume <span>\[
  \mathbb{E}(Y_i / n_i) = p_i = g^{-1}(\eta_i),
\]</span> where <span>\[
   \eta_i = x_i^T \beta
\]</span> is the <strong>linear predictor</strong> (or <strong>systematic component</strong>) and <span>\(g\)</span> is a <strong>link function</strong> that maps the range of <span>\(p_i\)</span> to the range of <span>\(\eta_i\)</span>.</p>
<p>We are left the choice of link function <span>\(g: p \mapsto \eta\)</span> that is monotone and <span>\(0 \le g^{-1}(\eta) \le 1\)</span> for any <span>\(\eta\)</span>. There are 3 common choices:</p>
<table>
<thead>
<tr>
<th></th>
<th>Link function <span>\(g\)</span></th>
<th>Inverse link function <span>\(g^{-1}\)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td>logit</td>
<td><span>\(\eta = \log \frac{p}{1-p}\)</span></td>
<td><span>\(p = \frac{e^\eta}{1 + e^\eta}\)</span></td>
</tr>
<tr>
<td>probit</td>
<td><span>\(\eta = \Phi^{-1}(p)\)</span></td>
<td><span>\(p = \Phi(\eta)\)</span></td>
</tr>
<tr>
<td>cloglog</td>
<td><span>\(\eta = \log ( - \log (1 - p))\)</span></td>
<td><span>\(p = 1 - e^{- e^\eta}\)</span></td>
</tr>
</tbody>
</table>
<p>Bionomial regression with logit link is called the <strong>logistic regression</strong>. For probit, <span>\(\Phi\)</span> is the cumulative distribution function (cdf) of standard normal.</p>
<div>
<h2>Parameter estimation, fitted values, and prediction</h2>
<p>Now the model is completely specified and we shall use the method of maximum likelihood (MLE) to estiamte the parameters <span>\(\beta\)</span>. The log-likelihood is given by<br>
<span>\[
  L(\beta) = \sum_{i=1}^n \left[ y_i \eta_i - n_i \log(1 + e^{\eta_i}) + \log \binom{n_i}{y_i} \right].
\]</span> We use R to estimate the regression parameters for the <em>logit</em> model.</p>
<pre><code>logitmod &lt;- glm(cbind(damage, 6 - damage) ~ temp, family = binomial, orings)
summary(logitmod)</code></pre>
<pre><code>## 
## Call:
## glm(formula = cbind(damage, 6 - damage) ~ temp, family = binomial, 
##     data = orings)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9529  -0.7345  -0.4393  -0.2079   1.9565  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) 11.66299    3.29626   3.538 0.000403 ***
## temp        -0.21623    0.05318  -4.066 4.78e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 38.898  on 22  degrees of freedom
## Residual deviance: 16.912  on 21  degrees of freedom
## AIC: 33.675
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>How about the <em>probit</em> model?</p>
<pre><code>probitmod &lt;- glm(cbind(damage, 6 - damage) ~ temp, family = binomial(link = probit), orings)
summary(probitmod)</code></pre>
<pre><code>## 
## Call:
## glm(formula = cbind(damage, 6 - damage) ~ temp, family = binomial(link = probit), 
##     data = orings)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.0134  -0.7761  -0.4467  -0.1581   1.9983  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  5.59145    1.71055   3.269  0.00108 ** 
## temp        -0.10580    0.02656  -3.984 6.79e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 38.898  on 22  degrees of freedom
## Residual deviance: 18.131  on 21  degrees of freedom
## AIC: 34.893
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>Although the coefficients seem quite different, the fitted values are similar, particularly in the range of the data.</p>
<pre><code>plot(damage / 6 ~ temp, orings, xlim = c(25, 85), ylim = c(0, 1), xlab = "Temperature", ylab = "Prob of damage")
x &lt;- seq(25, 85, 1)
lines(x, ilogit(11.6630 - 0.2162 * x))
lines(x, pnorm(5.5915 - 0.1058 * x), lty = 2)</code></pre>
<p><img width="672"></p>
<p>We can predict the response at 31°F for both models:</p>
<pre><code>ilogit (11.6630 - 0.2162 * 31)</code></pre>
<pre><code>## [1] 0.9930414</code></pre>
<pre><code>pnorm(5.5915 - 0.1058 * 31)</code></pre>
<pre><code>## [1] 0.9896029</code></pre>
</div>
<div>
<h2>Inference</h2>
<p>The <em>goodness of fit</em> of the fitted model can be measured by comparing to the <em>full</em> or <em>saturated</em> model that has as many parameters as number of observations and has fitted values <span>\(\hat p_i = y_i / n_i\)</span>. The likelihood ratio test (LRT) test statistics becomes <span>\[
  D = 2 \log \frac{L_{\text{larger model}}}{L_{\text{smaller model}}} = 2 \sum_{i=1}^n \left[ y_i \log \frac{y_i}{\hat y_i} + (n_i - y_i) \log \frac{n_i - y_i}{n_i - \hat y_i} \right],
\]</span> where <span>\(\hat y_i\)</span> are the fitted values from the smaller model. This test statistics is also called <strong>deviance</strong>, meaning how close the fitted model comes to perfection. When <span>\(Y\)</span> is truely binomial and <span>\(n_i\)</span> are relative large, the deviance is approximately <span>\(\chi^2\)</span> distributed with <span>\(n-p\)</span> degrees of freedom if the model is correct. For the logit model of the Challenger data, we have</p>
<pre><code>deviance(logitmod) # residual deviance</code></pre>
<pre><code>## [1] 16.91228</code></pre>
<pre><code>df.residual(logitmod) # n - p</code></pre>
<pre><code>## [1] 21</code></pre>
<pre><code>pchisq(deviance(logitmod), df.residual(logitmod), lower = FALSE)</code></pre>
<pre><code>## [1] 0.7164099</code></pre>
<p>We conclude that this model fits the data sufficiently well. The null model (with just the intercept) has an inadequate fit</p>
<pre><code>pchisq(38.9, 22, lower = FALSE)</code></pre>
<pre><code>## [1] 0.01448877</code></pre>
<p>so we cannot ascribe the response to simple variation not dependent on any predictor.</p>
<p>For binary responses where <span>\(n_i=1\)</span>, <span>\(\chi^2\)</span> asymptotics breaks and we have to resort to other methods, e.g., the Hosmer-Lemeshow test, to assess the goodness of fit. Rule of thum is to use <span>\(\chi^2\)</span> test when <span>\(n_i \ge 5\)</span>.</p>
<p>We can use deviance to compare two nested models. It’s called <strong>analysis of deviance</strong>, although it’s nothing but the likelihood ratio test (LRT). For example,</p>
<pre><code>pchisq(38.9 - 16.9, 1, lower = FALSE)</code></pre>
<pre><code>## [1] 2.726505e-06</code></pre>
<p>shows that the fitted model is significantly better than the null model.</p>
<p>Confidence intervals for the regression parameters may be constructed using normal approximations for the parameter estimates. A <span>\(100(1 - \alpha)\)</span> confidence interval for <span>\(\beta_i\)</span> would be: <span>\[
  \hat \beta_i \pm z^{\alpha/2} \text{se}(\hat \beta_i),
\]</span> where <span>\(z^{\alpha / 2}\)</span> is a quantile from the normal distribution. Thus a 95% confidence interval for <span>\(\beta_1\)</span> in our model would be</p>
<pre><code>c(- 0.2162 - 1.96 * 0.0532, - 0.2162 + 1.96 * 0.0532)</code></pre>
<pre><code>## [1] -0.320472 -0.111928</code></pre>
</div>
<div>
<h2>Odds, odds ratio, risk</h2>
<p>Let <span>\(p\)</span> be the success probability, then the <strong>odds</strong> is <span>\[
  o = \frac{p}{1 - p}.
\]</span> In logistic regression, we have <span>\[
  \log (\text{odds}) = \log \left( \frac{p}{1 - p} \right) = \beta_0 + x_1 \beta_1 + \cdots + x_p \beta_p.
\]</span> Therefore regression coefficient <span>\(\beta_i\)</span> has a natural interpretation: how much does a unit change in <span>\(x_i\)</span> decreases or increases the <em>log odds</em> of success. No such interpretation exists for other links (probit, cloglog).</p>
<p>Let <span>\(p_1\)</span> be the success probability in condition 1 and <span>\(p_2\)</span> the success probability in condition 2. Then the <strong>odds ratio</strong> is <span>\[
  \text{OR} = \frac{p_1 / (1 - p_1)}{p_2 / (1 - p_2)}
\]</span> and the <strong>relative risk</strong> is <span>\[
  \text{relative risk} = \frac{p_1}{p_2}.
\]</span> For rare outcomes, i.e., <span>\(p_1, p_2\)</span> very small, the relative risk and odds ratio are similar. But for larger probabilities, differences can be substantial.</p>
</div>
<div>
<h2>Example: infant respiratory disease</h2>
<p>A study on the proportions of children developing bronchitis or pneumonia in their first year of life by type of feeding and sex.</p>
<pre><code>data(babyfood)
babyfood</code></pre>
<pre><code>##   disease nondisease  sex   food
## 1      77        381  Boy Bottle
## 2      19        128  Boy  Suppl
## 3      47        447  Boy Breast
## 4      48        336 Girl Bottle
## 5      16        111 Girl  Suppl
## 6      31        433 Girl Breast</code></pre>
<pre><code>xtabs(disease / (disease + nondisease) ~ sex + food, babyfood)</code></pre>
<pre><code>##       food
## sex        Bottle     Breast      Suppl
##   Boy  0.16812227 0.09514170 0.12925170
##   Girl 0.12500000 0.06681034 0.12598425</code></pre>
<p>Let’s fit and examine the logistic model</p>
<pre><code>mdl &lt;- glm(cbind(disease, nondisease) ~ sex + food, family = binomial, babyfood)
summary(mdl)</code></pre>
<pre><code>## 
## Call:
## glm(formula = cbind(disease, nondisease) ~ sex + food, family = binomial, 
##     data = babyfood)
## 
## Deviance Residuals: 
##       1        2        3        4        5        6  
##  0.1096  -0.5052   0.1922  -0.1342   0.5896  -0.2284  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -1.6127     0.1124 -14.347  &lt; 2e-16 ***
## sexGirl      -0.3126     0.1410  -2.216   0.0267 *  
## foodBreast   -0.6693     0.1530  -4.374 1.22e-05 ***
## foodSuppl    -0.1725     0.2056  -0.839   0.4013    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 26.37529  on 5  degrees of freedom
## Residual deviance:  0.72192  on 2  degrees of freedom
## AIC: 40.24
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>We see breast-fed and, to a lesser extent, supplement-fed babies are less vulnerable to respiratory disease. We also see that boys are more vulnerable than girls. <code>drop1</code> function tests the significance of each factor relative to the full. We see both factors are significant.</p>
<pre><code>drop1(mdl, test = "Chi")</code></pre>
<pre><code>## Single term deletions
## 
## Model:
## cbind(disease, nondisease) ~ sex + food
##        Df Deviance    AIC     LRT  Pr(&gt;Chi)    
## &lt;none&gt;      0.7219 40.240                      
## sex     1   5.6990 43.217  4.9771   0.02569 *  
## food    2  20.8992 56.417 20.1772 4.155e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<p>To intepret the regression coefficients,</p>
<pre><code>exp(-0.6693)</code></pre>
<pre><code>## [1] 0.5120669</code></pre>
<p>We see that breast feeding reduces the odds of respiratory disease to 51% of that for bottle feeding. A confidence on the log-odds can be constructed</p>
<pre><code>exp(c(- 0.669 - 1.96 * 0.153, - 0.669 + 1.96 * 0.153))</code></pre>
<pre><code>## [1] 0.3795078 0.6913424</code></pre>
</div>
<div>
<h2>Prospective and retrospective sampling</h2>
<p><strong>Prospective sampling</strong>: predictors are fixed and then outcome is observed. It’s also called <strong>cohort study</strong>.</p>
<p><strong>Retrospective sampling</strong>: the outcome is fixed and then the predictors are observed. We require the probability of inclusion in the study is independent of the predictor values. This is also called a <strong>case-control study</strong>.</p>
<p>Let’s focus on just boys who are breast or bottle fed. The data is</p>
<pre><code>babyfood[c(1, 3), ]</code></pre>
<pre><code>##   disease nondisease sex   food
## 1      77        381 Boy Bottle
## 3      47        447 Boy Breast</code></pre>
<p>First assume the data is from the <em>prospective sampling</em>. Given the infant is breast fed, the log-odds of having a respiratory disease are <span>\[
  \log (47 / 447) = - 2.25.
\]</span> Given the infant is bottle fed, the log-odds of having a respiratory disease are <span>\[
  \log (77 / 381) = - 1.60.
\]</span> So the log-odds ratio <span>\(\Delta = (- 1.60) - (- 2.25) = 0.65\)</span> represents the increased risk of respiratory disease incurred by bottle feeding relative to breast feeding.</p>
<p>Suppose that this had been a <em>retrospective study</em>. Then we compute the log-odds of feeding type given respiratory disease status and then find the difference. This actually give the same result because <span>\[
  \Delta = \log (77 / 47) - \log (381 / 447) = \log (77 / 381) - \log (47 / 447) = 0.65
\]</span></p>
<p>This shows that a retrospective design is as effective as a prospective design for estimating log-odds ratio <span>\(\Delta\)</span>. Retrospective designs are cheaper, faster and more efficient, so it is convenient that the same result may be obtained from the prospective study. Probit and cloglog do <em>not</em> have such relations.</p>
</div>
<div>
<h2>Choice of link functions</h2>
<p>Let’s look at some data on the number of insects dying at different levels of insecticide concentration.</p>
<pre><code>data(bliss)
bliss</code></pre>
<pre><code>##   dead alive conc
## 1    2    28    0
## 2    8    22    1
## 3   15    15    2
## 4   23     7    3
## 5   27     3    4</code></pre>
<p>We fit all three link functions.</p>
<pre><code>modl &lt;- glm(cbind(dead, alive) ~ conc, family = binomial, data = bliss)
modp &lt;- glm(cbind(dead, alive) ~ conc, family = binomial(link = probit), data = bliss)
modc &lt;- glm(cbind(dead, alive) ~ conc, family = binomial(link = cloglog), data = bliss)</code></pre>
<p>We compare the fitted values from 3 link functions</p>
<pre><code>cbind(fitted(modl), fitted(modp), fitted(modc))</code></pre>
<pre><code>##         [,1]       [,2]      [,3]
## 1 0.08917177 0.08424186 0.1272700
## 2 0.23832314 0.24487335 0.2496909
## 3 0.50000000 0.49827210 0.4545910
## 4 0.76167686 0.75239612 0.7217655
## 5 0.91082823 0.91441122 0.9327715</code></pre>
<p>These are not very different, but if we extrapolate to a wider range</p>
<pre><code>x &lt;- seq(-2, 8, 0.2)
pl &lt;- ilogit(modl$coef[1] + modl$coef[2] * x)
pp &lt;- pnorm(modp$coef[1] + modp$coef[2] * x)
pc &lt;- 1 - exp(-exp((modc$coef[1] + modc$coef[2] * x)))
plot(x, pl, type = "l", ylab = "Probability", xlab = "Dose") # logit: solid line
lines(x, pp, lty = 2) # probit: dotted line
lines(x, pc, lty = 5) # cloglog: dashed line</code></pre>
<p><img width="672"></p>
<p>Now we look at the relative differences</p>
<pre><code>matplot(x, cbind(pp / pl, (1 - pp) / (1 - pl)), type = "l", xlab = "Dose", ylab = "Ratio")</code></pre>
<p><img width="672"></p>
<pre><code>matplot(x, cbind(pc / pl, (1 - pc) / (1 - pl)), type = "l", xlab = "Dose", ylab = "Ratio")</code></pre>
<p><img width="672"></p>
<p>The differences are substantial in the tail probabilities when we extrapolate. Unfortunately this is also the region we don’t have much data to differentiate them.</p>
<p>In summary, the default choice is the logit link. There are several advantages: it leads to simpler mathematics, better numerical performance, and it is easier to interpret using odds, and it allows easier analysis of retrospectively sampled data.</p>
</div>
<div>
<h2>Confidence interval on prediction</h2>
<p>For a given covariate <span>\(x_0\)</span>, <span>\(\hat \eta = x_0^T \hat \beta\)</span> with variance <span>\(x_0^T (X^T W X)^{-1} x_0\)</span>. Approximate confidence intervals can be obtained using a normal approximation. To get an answer in the probability scale, we need to transform back using the inverse link function. For the insect data,</p>
<pre><code>modl &lt;- glm(cbind(dead, alive) ~ conc, family = binomial, data = bliss)
lmodsum &lt;- summary(modl)
lmodsum</code></pre>
<pre><code>## 
## Call:
## glm(formula = cbind(dead, alive) ~ conc, family = binomial, data = bliss)
## 
## Deviance Residuals: 
##       1        2        3        4        5  
## -0.4510   0.3597   0.0000   0.0643  -0.2045  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -2.3238     0.4179  -5.561 2.69e-08 ***
## conc          1.1619     0.1814   6.405 1.51e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 64.76327  on 4  degrees of freedom
## Residual deviance:  0.37875  on 3  degrees of freedom
## AIC: 20.854
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>To predict response at dose of 2.5</p>
<pre><code>x0 &lt;- c(1, 2.5)
eta0 &lt;- sum(x0 * coef(modl))
ilogit(eta0)</code></pre>
<pre><code>## [1] 0.6412854</code></pre>
<p>A 95% confidence interval is constructed by</p>
<pre><code>cm &lt;- lmodsum$cov.unscaled
cm</code></pre>
<pre><code>##             (Intercept)        conc
## (Intercept)  0.17463024 -0.06582336
## conc        -0.06582336  0.03291168</code></pre>
<pre><code>se &lt;- sqrt(t(x0) %*% cm %*% x0)
se</code></pre>
<pre><code>##           [,1]
## [1,] 0.2262995</code></pre>
<pre><code>ilogit(c(eta0 - 1.96 * se, eta0 + 1.96 * se))</code></pre>
<pre><code>## [1] 0.5342962 0.7358471</code></pre>
<p>A more direct way is</p>
<pre><code>predict(modl, newdata = data.frame(conc = 2.5), se = T)</code></pre>
<pre><code>## $fit
##         1 
## 0.5809475 
## 
## $se.fit
## [1] 0.2262995
## 
## $residual.scale
## [1] 1</code></pre>
<pre><code>ilogit(c(0.58095 - 1.96 * 0.2263, 0.58095 + 1.96 * 0.2263))</code></pre>
<pre><code>## [1] 0.5342966 0.7358478</code></pre>
</div>
<div>
<h2>Separation and convergence issues</h2>
<p>In general, parameters of binomial regression are estimated by the Fisher scoring algorithm. Sometimes we encounter convergence issues, as shown in the following data on predicting sex orientation of 26 males based on hormone levels. <code>s</code> indicates heterosexual. <code>g</code> means homosexual.</p>
<pre><code>data(hormone)
hormone</code></pre>
<pre><code>##    androgen estrogen orientation
## 1       3.9      1.8           s
## 2       4.0      2.3           s
## 3       3.8      2.3           s
## 4       3.9      2.5           s
## 5       2.9      1.3           s
## 6       3.2      1.7           s
## 7       4.6      3.4           s
## 8       4.3      3.1           s
## 9       3.1      1.8           s
## 10      2.7      1.5           s
## 11      2.3      1.4           s
## 12      2.5      2.1           g
## 13      1.6      1.1           g
## 14      3.9      3.9           g
## 15      3.4      3.6           g
## 16      2.3      2.5           g
## 17      1.6      1.7           g
## 18      2.5      2.9           g
## 19      3.4      4.0           g
## 20      1.6      1.9           g
## 21      4.3      5.3           g
## 22      2.0      2.7           g
## 23      1.8      3.6           g
## 24      2.2      4.1           g
## 25      3.1      5.2           g
## 26      1.3      4.0           g</code></pre>
<pre><code>#plot(estrogen ~ androgen, data = hormone, pch = as.character(orientation))</code></pre>
<p>Convergence issue occurs when fitting the logistic regression</p>
<pre><code>modl &lt;- glm(orientation ~ estrogen + androgen, hormone, family = binomial)</code></pre>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<p>We see the parameter estimates are large and the residual deviance is extremely small</p>
<pre><code>summary(modl)</code></pre>
<pre><code>## 
## Call:
## glm(formula = orientation ~ estrogen + androgen, family = binomial, 
##     data = hormone)
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -2.759e-05  -2.100e-08  -2.100e-08   2.100e-08   3.380e-05  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)    -84.49  136095.03  -0.001    1.000
## estrogen       -90.22   75910.98  -0.001    0.999
## androgen       100.91   92755.62   0.001    0.999
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 3.5426e+01  on 25  degrees of freedom
## Residual deviance: 2.3229e-09  on 23  degrees of freedom
## AIC: 6
## 
## Number of Fisher Scoring iterations: 25</code></pre>
<pre><code>plot(estrogen ~ androgen, data = hormone, pch = as.character(orientation))
abline(- 84.5 / 90.2, 100.9 / 90.2)</code></pre>
<p><img width="672"></p>
<p>Looking at the data again reveals orientation can be perfectly separated. This is one example of <strong>separation</strong> where we get a perfect fit of the data. The MLE occurs at infinity along the separating line.</p>
</div>
<div>
<h2>Overdispersion</h2>
<p>Under binomial model, the variance of a response with batch size <span>\(m\)</span> is <span>\[
  \mathbb{Var}(Y) = m p (1 - p).
\]</span> In many applications, data exhibits larger or, to a lesser extent, smaller variance due to several reasons:</p>
<ul>
<li>heterogeneity in the success probability <span>\(p\)</span> of each trial<br>
</li>
<li>dependence between trials</li>
</ul>
<p>The simplest approach is to introduce an additional dispersion parameter <span>\(\phi\)</span>. For Gaussian model, <span>\(\phi\)</span> is just <span>\(\sigma^2\)</span>. In the standard binomial case <span>\(\phi = 1\)</span>. We now let <span>\(\phi\)</span> vary and estimate using the data <span>\[
  \hat \phi = \frac{X^2}{n - p},
\]</span> where <span>\[
  X^2 = \sum_{i=1}^n \frac{(O_i - E_i)^2}{E_i} = \sum_{i=1}^n \frac{(y_i - n_i \hat p_i)^2}{n_i \hat p_i (1 - \hat p_i)}
\]</span> is the Pearson’s <span>\(X^2\)</span> statistic. The estimation of <span>\(\beta\)</span> is unaffected since <span>\(\sigma^2\)</span> does not change the mean response but <span>\[
  \text{Var} (\hat \beta) = \hat \phi (X^T W X)^{-1}.
\]</span> We cannot use the difference in deviances when comparing models, because the test statistic will be distributed <span>\(\phi \chi^2\)</span>. Since <span>\(\phi\)</span> is unknown and is estimated, we can use an F test <span>\[
  F = \frac{(D_{\text{small}} - D_{\text{large}})/(\text{df}_{<wbr>\text{small}} - \text{df}_{\text{large}})}{\<wbr>hat \phi}.
\]</span></p>
<p>Let’s take a look at an experiment</p>
<pre><code>data(troutegg)
troutegg</code></pre>
<pre><code>##    survive total location period
## 1       89    94        1      4
## 2      106   108        2      4
## 3      119   123        3      4
## 4      104   104        4      4
## 5       49    93        5      4
## 6       94    98        1      7
## 7       91   106        2      7
## 8      100   130        3      7
## 9       80    97        4      7
## 10      11   113        5      7
## 11      77    86        1      8
## 12      87    96        2      8
## 13      88   119        3      8
## 14      67    99        4      8
## 15      18    88        5      8
## 16     141   155        1     11
## 17     104   122        2     11
## 18      91   125        3     11
## 19     111   132        4     11
## 20       0   138        5     11</code></pre>
<pre><code>ftable(xtabs(cbind(survive, total) ~ location + period, troutegg))</code></pre>
<pre><code>##                  survive total
## location period               
## 1        4            89    94
##          7            94    98
##          8            77    86
##          11          141   155
## 2        4           106   108
##          7            91   106
##          8            87    96
##          11          104   122
## 3        4           119   123
##          7           100   130
##          8            88   119
##          11           91   125
## 4        4           104   104
##          7            80    97
##          8            67    99
##          11          111   132
## 5        4            49    93
##          7            11   113
##          8            18    88
##          11            0   138</code></pre>
<pre><code>bmod &lt;- glm(cbind(survive, total - survive) ~ location + period, family = binomial, troutegg)
summary(bmod)</code></pre>
<pre><code>## 
## Call:
## glm(formula = cbind(survive, total - survive) ~ location + period, 
##     family = binomial, data = troutegg)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -4.8305  -0.3650  -0.0303   0.6191   3.2434  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   4.6358     0.2813  16.479  &lt; 2e-16 ***
## location2    -0.4168     0.2461  -1.694   0.0903 .  
## location3    -1.2421     0.2194  -5.660 1.51e-08 ***
## location4    -0.9509     0.2288  -4.157 3.23e-05 ***
## location5    -4.6138     0.2502 -18.439  &lt; 2e-16 ***
## period7      -2.1702     0.2384  -9.103  &lt; 2e-16 ***
## period8      -2.3256     0.2429  -9.573  &lt; 2e-16 ***
## period11     -2.4500     0.2341 -10.466  &lt; 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1021.469  on 19  degrees of freedom
## Residual deviance:   64.495  on 12  degrees of freedom
## AIC: 157.03
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>The deviance of 64.5 on 12 degrees of freedom indicates lack of fit. We can estimate the dispersion parameter as</p>
<pre><code>sigma2 &lt;- sum(residuals(bmod, type = "pearson")^2) / 12
sigma2</code></pre>
<pre><code>## [1] 5.330322</code></pre>
<p>which is substantially large than 1. F-test is</p>
<pre><code>drop1(bmod, scale = sigma2, test = "F")</code></pre>
<pre><code>## Warning in drop1.glm(bmod, scale = sigma2, test = "F"): F test assumes
## 'quasibinomial' family</code></pre>
<pre><code>## Single term deletions
## 
## Model:
## cbind(survive, total - survive) ~ location + period
## 
## scale:  5.330322 
## 
##          Df Deviance    AIC F value    Pr(&gt;F)    
## &lt;none&gt;         64.50 157.03                      
## location  4   913.56 308.32  39.494 8.142e-07 ***
## period    3   228.57 181.81  10.176  0.001288 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
<p>We can use the dispersion parameter to scale up the estimates of the standard error</p>
<pre><code>summary(bmod, dispersion = sigma2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = cbind(survive, total - survive) ~ location + period, 
##     family = binomial, data = troutegg)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -4.8305  -0.3650  -0.0303   0.6191   3.2434  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   4.6358     0.6495   7.138 9.49e-13 ***
## location2    -0.4168     0.5682  -0.734   0.4632    
## location3    -1.2421     0.5066  -2.452   0.0142 *  
## location4    -0.9509     0.5281  -1.800   0.0718 .  
## location5    -4.6138     0.5777  -7.987 1.39e-15 ***
## period7      -2.1702     0.5504  -3.943 8.05e-05 ***
## period8      -2.3256     0.5609  -4.146 3.38e-05 ***
## period11     -2.4500     0.5405  -4.533 5.82e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 5.330322)
## 
##     Null deviance: 1021.469  on 19  degrees of freedom
## Residual deviance:   64.495  on 12  degrees of freedom
## AIC: 157.03
## 
## Number of Fisher Scoring iterations: 5</code></pre>
</div>
</div>
<div>
<h1>Multinomial regression</h1>
<p>Multinomial regression is a natural extension of the binomial regression to the case when responses are multivaraite counts. Let <span>\(Y_{ij}\)</span> be the number of observations falling into category <span>\(j\)</span> for batch <span>\(i\)</span> and let <span>\(n_i = \sum_j Y_{ij}\)</span> be the batch size. Then <span>\[
  \mathbb{P}(Y_{i1} = y_{i1}, \ldots, Y_{iJ} = y_{iJ}) = \binom{n_i}{y_{i1} \cdots y_{iJ}} p_{i1}^{y_{i1}} \cdots p_{iJ}^{y_{iJ}}.
\]</span></p>
<div>
<h2>Multinomial logit model</h2>
<p>We distinguish between <em>nominal</em> multinomial data where there is no natural order to the categories and <em>ordinal</em> multinomial data where there is an order. The <em>multinomial logit</em> model is indended for norminal data. It can be used for ordinal data, but the informtion about order will not be used.</p>
<p>We use the same idea as the logistic regression. <span>\[
  \eta_{ij} = x_i^T \beta_j = \log \frac{p_{ij}}{p_{i1}}
\]</span> We need to obey the constraint that <span>\(\sum_{i=1}^J p_{ij} = 1\)</span> so it is convenient to declare one of the categories as the baseline say <span>\(j=1\)</span>. So we set <span>\(p_{i1} = 1 - \sum_{j=2}^J p_{ij}\)</span> and have <span>\[
  p_{ij} = \frac{\exp(\eta_{ij})}{1 + \sum_{j=2}^J \exp(\eta_{ij})}.
\]</span> We may estimate the parameters using MLE and the standard methods for inference.</p>
<p>Consider an example of the 1996 American National Election Study. We consider only the education level and income group of the respondents. Our response will be party identification of the respondent: Democrat, Independent or Republican. The income variable in the original data was an ordered factor with income ranges. We have converted this to a numeric variable by taking the midpoint of each range.</p>
<pre><code>data(nes96)
head(nes96, 20)</code></pre>
<pre><code>##    popul TVnews selfLR ClinLR DoleLR     PID age   educ   income    vote
## 1      0      7 extCon extLib    Con  strRep  36     HS $3Kminus    Dole
## 2    190      1 sliLib sliLib sliCon weakDem  20   Coll $3Kminus Clinton
## 3     31      7    Lib    Lib    Con weakDem  24  BAdeg $3Kminus Clinton
## 4     83      4 sliLib    Mod sliCon weakDem  28  BAdeg $3Kminus Clinton
## 5    640      7 sliCon    Con    Mod  strDem  68  BAdeg $3Kminus Clinton
## 6    110      3 sliLib    Mod    Con weakDem  21   Coll $3Kminus Clinton
## 7    100      7 sliCon    Con    Mod weakDem  77   Coll $3Kminus Clinton
## 8     31      1 sliCon    Mod sliCon  indRep  21   Coll $3Kminus Clinton
## 9    180      7    Mod    Con sliLib  indind  31   Coll $3Kminus Clinton
## 10  2800      0 sliLib sliLib extCon  strDem  39     HS $3Kminus Clinton
## 11  1600      0 sliLib    Lib    Mod  indRep  26 HSdrop $3Kminus Clinton
## 12   330      5    Mod sliLib    Con weakDem  31   Coll $3Kminus Clinton
## 13   190      2 sliCon    Mod    Con weakRep  22   Coll $3Kminus    Dole
## 14   100      7    Mod    Mod    Con  strDem  42  CCdeg $3Kminus Clinton
## 15  1000      7 sliCon extCon    Mod  strDem  74     MS $3Kminus Clinton
## 16     0      7    Con extCon sliCon  strDem  62     HS $3Kminus Clinton
## 17   130      7    Mod    Mod sliCon weakDem  58     HS $3Kminus Clinton
## 18     5      5 sliLib sliLib    Con weakDem  24  BAdeg $3Kminus Clinton
## 19    33      7    Con    Lib    Con weakRep  51   Coll $3Kminus    Dole
## 20    19      2    Lib extLib    Mod  strDem  36     HS  $3K-$5K Clinton</code></pre>
<pre><code>sPID &lt;- nes96$PID
levels(sPID) &lt;- c("Democrat", "Democrat", "Independent", "Independent", "Independent", "Republican", "Republican")
summary(sPID)</code></pre>
<pre><code>##    Democrat Independent  Republican 
##         380         239         325</code></pre>
<pre><code>#unclass(nes96$income)
inca &lt;- c(1.5, 4, 6, 8, 9.5, 10.5, 11.5, 12.5, 13.5, 14.5, 16, 18.5, 21, 23.5,
 27.5, 32.5, 37.5, 42.5, 47.5, 55, 67.5, 82.5, 97.5, 115)
nincome &lt;- inca[unclass(nes96$income)]
summary(nincome)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.50   23.50   37.50   46.58   67.50  115.00</code></pre>
<pre><code>table(nes96$educ)</code></pre>
<pre><code>## 
##     MS HSdrop     HS   Coll  CCdeg  BAdeg  MAdeg 
##     13     52    248    187     90    227    127</code></pre>
<p>First we do some descriptive statistics.</p>
<pre><code># black: democratic, red: independent, green: republic
matplot(prop.table(table(<wbr>nes96$educ, sPID), 1), type = "l", xlab = "Education", ylab = "Proportion", lty = c(1, 2, 5))</code></pre>
<p><img width="672"></p>
<pre><code>cutinc &lt;- cut(nincome, 7)
il &lt;- c(8, 26, 42, 58, 74, 90, 107)
matplot(il, prop.table(table(cutinc, sPID), 1), lty = c(1, 2, 5), type = "l", ylab = "Proportion", xlab = "Income")</code></pre>
<p><img width="672"></p>
<pre><code>cutage &lt;- cut(nes96$age, 7)
al &lt;- c(24, 34, 44, 54, 65, 75, 85)
matplot(al, prop.table(table(cutage, sPID), 1), lty = c(1, 2, 5), type = "l", ylab = "Proportion", xlab = "Age")</code></pre>
<p><img width="672"></p>
<p>To verify the trends observecd in these graphs are statistical significant, we need a formal model. The <code>multinom</code> function is avaible in the <code>nnet</code> package.</p>
<pre><code>library(nnet)
mmod &lt;- multinom(sPID ~ age + educ + nincome, nes96)</code></pre>
<pre><code>## # weights:  30 (18 variable)
## initial  value 1037.090001 
## iter  10 value 990.568608
## iter  20 value 984.319052
## final  value 984.166272 
## converged</code></pre>
<pre><code>summary(mmod)</code></pre>
<pre><code>## Call:
## multinom(formula = sPID ~ age + educ + nincome, data = nes96)
## 
## Coefficients:
##             (Intercept)          age     educ.L     educ.Q    educ.C
## Independent   -1.197260 0.0001534525 0.06351451 -0.1217038 0.1119542
## Republican    -1.642656 0.0081943691 1.19413345 -1.2292869 0.1544575
##                  educ^4     educ^5      educ^6    nincome
## Independent -0.07657336  0.1360851  0.15427826 0.01623911
## Republican  -0.02827297 -0.1221176 -0.03741389 0.01724679
## 
## Std. Errors:
##             (Intercept)         age    educ.L    educ.Q    educ.C
## Independent   0.3265951 0.005374592 0.4571884 0.4142859 0.3498491
## Republican    0.3312877 0.004902668 0.6502670 0.6041924 0.4866432
##                educ^4    educ^5    educ^6     nincome
## Independent 0.2883031 0.2494706 0.2171578 0.003108585
## Republican  0.3605620 0.2696036 0.2031859 0.002881745
## 
## Residual Deviance: 1968.333 
## AIC: 2004.333</code></pre>
<p>We can select which variables to include in the model based the AIC criterion using a stepwise search method.</p>
<pre><code>mmodi &lt;- step(mmod)</code></pre>
<pre><code>## Start:  AIC=2004.33
## sPID ~ age + educ + nincome
## 
## trying - age 
## # weights:  27 (16 variable)
## initial  value 1037.090001 
## iter  10 value 988.896864
## iter  20 value 985.822223
## final  value 985.812737 
## converged
## trying - educ 
## # weights:  12 (6 variable)
## initial  value 1037.090001 
## iter  10 value 992.269502
## final  value 992.269484 
## converged
## trying - nincome 
## # weights:  27 (16 variable)
## initial  value 1037.090001 
## iter  10 value 1009.025560
## iter  20 value 1006.961593
## final  value 1006.955275 
## converged
##           Df      AIC
## - educ     6 1996.539
## - age     16 2003.625
## &lt;none&gt;    18 2004.333
## - nincome 16 2045.911
## # weights:  12 (6 variable)
## initial  value 1037.090001 
## iter  10 value 992.269502
## final  value 992.269484 
## converged
## 
## Step:  AIC=1996.54
## sPID ~ age + nincome
## 
## trying - age 
## # weights:  9 (4 variable)
## initial  value 1037.090001 
## final  value 992.712152 
## converged
## trying - nincome 
## # weights:  9 (4 variable)
## initial  value 1037.090001 
## final  value 1020.425203 
## converged
##           Df      AIC
## - age      4 1993.424
## &lt;none&gt;     6 1996.539
## - nincome  4 2048.850
## # weights:  9 (4 variable)
## initial  value 1037.090001 
## final  value 992.712152 
## converged
## 
## Step:  AIC=1993.42
## sPID ~ nincome
## 
## trying - nincome 
## # weights:  6 (2 variable)
## initial  value 1037.090001 
## final  value 1020.636052 
## converged
##           Df      AIC
## &lt;none&gt;     4 1993.424
## - nincome  2 2045.272</code></pre>
<p>Or we can use the standard likelihood methods to derive a test to compare nested models. For example, we can fit a model without education and then compare the deviances</p>
<pre><code>mmode &lt;- multinom(sPID ~ age + nincome, nes96)</code></pre>
<pre><code>## # weights:  12 (6 variable)
## initial  value 1037.090001 
## iter  10 value 992.269502
## final  value 992.269484 
## converged</code></pre>
<pre><code>deviance(mmode) - deviance(mmod)</code></pre>
<pre><code>## [1] 16.20642</code></pre>
<pre><code>pchisq(16.206, mmod$edf - mmode$edf, lower = F)</code></pre>
<pre><code>## [1] 0.181982</code></pre>
<p>We see education is not significant relative to the full model. This may seem somewhat surprising given the plot, but the large differences between proportions of Democrats and Republicans occur for groups with low education which represent only a small number of people.</p>
<p>We can obtain predicted values for specified values of income. For example, suppose we pick the midpoints of the income groups we selected for the earlier plot</p>
<pre><code>predict(mmodi, data.frame(nincome = il), type = "probs")</code></pre>
<pre><code>##    Democrat Independent Republican
## 1 0.5566253   0.1955183  0.2478565
## 2 0.4804946   0.2254595  0.2940459
## 3 0.4134268   0.2509351  0.3356381
## 4 0.3493884   0.2743178  0.3762939
## 5 0.2903271   0.2948600  0.4148129
## 6 0.2375755   0.3121136  0.4503109
## 7 0.1891684   0.3266848  0.4841468</code></pre>
</div>
<div>
<h2>Hierarchical or nested responses</h2>
<p>Consider following data concerning live births with deformations of the central nervous system (CNS) in south Wales.</p>
<pre><code>data(cns)
cns</code></pre>
<pre><code>##             Area NoCNS An Sp Other Water      Work
## 1        Cardiff  4091  5  9     5   110 NonManual
## 2        Newport  1515  1  7     0   100 NonManual
## 3        Swansea  2394  9  5     0    95 NonManual
## 4     GlamorganE  3163  9 14     3    42 NonManual
## 5     GlamorganW  1979  5 10     1    39 NonManual
## 6     GlamorganC  4838 11 12     2   161 NonManual
## 7      MonmouthV  2362  6  8     4    83 NonManual
## 8  MonmouthOther  1604  3  6     0   122 NonManual
## 9        Cardiff  9424 31 33    14   110    Manual
## 10       Newport  4610  3 15     6   100    Manual
## 11       Swansea  5526 19 30     4    95    Manual
## 12    GlamorganE 13217 55 71    19    42    Manual
## 13    GlamorganW  8195 30 44    10    39    Manual
## 14    GlamorganC  7803 25 28    12   161    Manual
## 15     MonmouthV  9962 36 37    13    83    Manual
## 16 MonmouthOther  3172  8 13     3   122    Manual</code></pre>
<p>Responses:</p>
<ul>
<li>NoCNS: no CNS<br>
</li>
<li>An: anencephalus<br>
</li>
<li>Sp: sina bifida<br>
</li>
<li>Other: other malformations</li>
</ul>
<p>Predictors:</p>
<ul>
<li>Water: water hardness<br>
</li>
<li>Work: type of work performed by the parents</li>
</ul>
<p>We might consider a multinomial response with four categories. However, we can see that most births suffer no malformation and so this category dominates the other three. It is better to consider this as a hierarchical response. Consider the multinomial likelihood for the <span>\(i\)</span>-th observation which is proportional to: <span>\[
  p_{i1}^{y_{i1}} p_{i2}^{y_{i2}} p_{i3}^{y_{i3}} p_{i4}^{y_{i4}}
\]</span> Define <span>\(p_{ic} = p_{i2} + p_{i3} + p_{i4}\)</span> which is the probability of a birth with some kind of CNS malformation. We can then write the likelihood as <span>\[
  p_{i1}^{y_{i1}} p_{ic}^{y_{i2} + y_{i3} + y_{i4}} \times \left( \frac{p_{i2}}{p_{ic}} \right)^{y_{i2}} \left( \frac{p_{i3}}{p_{ic}} \right)^{y_{i3}} \left( \frac{p_{i4}}{p_{ic}} \right)^{y_{i4}}
\]</span> We can now separatly develop a binomial model for whether malformation occurs and a multinomial model for the type of malformation.</p>
</div>
<div>
<h2>Ordinal multinomial responses</h2>
<p>Suppose we have <span>\(J\)</span> ordered categories and <span>\(p_{ij} = \mathbb{P}(Y_i = j)\)</span> for <span>\(j=1, \ldots, J\)</span>. For ordered responses, it is more convenient to work with the cumulative probabilities <span>\[
  \gamma_{ij} = \mathbb{P}(Y_i \le j).
\]</span> Since <span>\(\gamma_{iJ}=1\)</span>, we only need to model <span>\(J-1\)</span> cumulative probabilities.</p>
<p>To link <span>\(\gamma\)</span>s to covariats <span>\(x\)</span>, we consider <span>\[
  g(\gamma_{ij}) = \theta_j - x_i^T \beta,
\]</span> where <span>\(\theta_j\)</span> have to be non-decreasing in <span>\(j\)</span> to honor the ordering. In other words, <span>\(\beta\)</span> has a uniform effect on the response categories and each category has its own intercept. Again we have at least 3 choices for the link function: logit, probit or cloglog.</p>
<div>
<h3>Proportional odds model</h3>
<p>The mostly common logit link dictates <span>\[
  \gamma_{ij} = \frac{\exp (\theta_j - x_i^T \beta)}{1 + \exp (\theta_j - x_i^T \beta)}.
\]</span> When <span>\(\beta &gt; 0\)</span>, as <span>\(x_i\)</span> increases, <span>\(\mathbb{P}(Y_i = J)\)</span> will also increase. This motivats the minus sign in the definition of the model since it allows easier interpretation of <span>\(\beta\)</span>.</p>
<p>Let <span>\(\gamma_j(x_i) = \mathbb{P}(Y_i \le j \mid x_i)\)</span>, then <span>\[
  \log \frac{\gamma_j(x_i)}{1 - \gamma_j(x_i)} = \theta_j - x_i^T \beta, \quad j = 1,\ldots,J-1.
\]</span></p>
<p>It is called the <em>proportional odds model</em> because the relative odds for <span>\(y \le j\)</span> comparing <span>\(x_1\)</span> and <span>\(x_2\)</span> are <span>\[
  \left( \frac{\gamma_j(x_1)}{1 - \gamma_j(x_1)} \right) / \left( \frac{\gamma_j(x_2)}{1 - \gamma_j(x_2)} \right) = \exp (- (x_1 - x_2)^T \beta),
\]</span> which do not dependent on <span>\(j\)</span>.</p>
<p>We can fit a proportional odds model using the <code>polr</code> function from the MASS library</p>
<pre><code>library(MASS)
pomod &lt;- polr(sPID ~ age + educ + nincome, nes96)
summary(pomod)</code></pre>
<pre><code>## 
## Re-fitting to get Hessian</code></pre>
<pre><code>## Call:
## polr(formula = sPID ~ age + educ + nincome, data = nes96)
## 
## Coefficients:
##             Value Std. Error  t value
## age      0.005775   0.003887  1.48581
## educ.L   0.724087   0.384388  1.88374
## educ.Q  -0.781361   0.351173 -2.22500
## educ.C   0.040168   0.291762  0.13767
## educ^4  -0.019925   0.232429 -0.08573
## educ^5  -0.079413   0.191533 -0.41462
## educ^6  -0.061104   0.157747 -0.38735
## nincome  0.012739   0.002140  5.95187
## 
## Intercepts:
##                        Value   Std. Error t value
## Democrat|Independent    0.6449  0.2435     2.6479
## Independent|Republican  1.7374  0.2493     6.9694
## 
## Residual Deviance: 1984.211 
## AIC: 2004.211</code></pre>
<p>The deviance and number of parameters are</p>
<pre><code>c(deviance(pomod), pomod$edf)</code></pre>
<pre><code>## [1] 1984.211   10.000</code></pre>
<p>which can be compared to the corresponding multinomial logit model</p>
<pre><code>c(deviance(mmod), mmod$edf)</code></pre>
<pre><code>## [1] 1968.333   18.000</code></pre>
</div>
<div>
<h3>Ordered probit model</h3>
<p>If we use the probit link, then <span>\[
  \Phi^{-1}(\gamma_j(x_i)) = \theta_j - x_i^T \beta, \quad j=1,\ldots,J-1.
\]</span></p>
<pre><code>opmod &lt;- polr(sPID ~ nincome, method = "probit")
summary(opmod)</code></pre>
<pre><code>## 
## Re-fitting to get Hessian</code></pre>
<pre><code>## Call:
## polr(formula = sPID ~ nincome, method = "probit")
## 
## Coefficients:
##            Value Std. Error t value
## nincome 0.008182   0.001208   6.775
## 
## Intercepts:
##                        Value   Std. Error t value
## Democrat|Independent    0.1284  0.0694     1.8510
## Independent|Republican  0.7976  0.0722    11.0399
## 
## Residual Deviance: 1994.892 
## AIC: 2000.892</code></pre>
<p>The deviance is similar to the logit link, but the coefficients appear to be different. The predictions are similar.</p>
</div>
<div>
<h3>Proportional hazards model</h3>
<p>Suppose we use the cloglog link <span>\[
  \log (- \log (1 - \gamma_j(x_i))) = \theta_j - x_i^T \beta.
\]</span> The <em>hazard</em> of category <span>\(j\)</span> is the probability of falling in category <span>\(j\)</span> given that your category is greater than <span>\(j\)</span> <span>\[
  \text{Hazard}(j) = \mathbb{P}(Y_i = j \mid Y_i \ge j) = \frac{\mathbb{P}(Y_i = j)}{\mathbb{P}(Y_i \ge j)} = \frac{\gamma_{ij} - \gamma_{i,j-1}}{1 - \gamma_{i,j-1}}.
\]</span> It is called the <em>proportional hazards model</em> because of the relation <span>\[
  \mathbb{P}(Y &gt; j \mid x_1) = [\mathbb{P}(Y &gt; j \mid x_2)]^{\exp (x_1 - x_2)^T \beta}.
\]</span></p>
<pre><code>polr(sPID ~ nincome, method = "cloglog")</code></pre>
<pre><code>## Call:
## polr(formula = sPID ~ nincome, method = "cloglog")
## 
## Coefficients:
##     nincome 
## 0.008271135 
## 
## Intercepts:
##   Democrat|Independent Independent|Republican 
##             -0.2866703              0.4564409 
## 
## Residual Deviance: 2003.96 
## AIC: 2009.96</code></pre>
</div>
</div>
</div>
<div>
<h1>GLM</h1>
<p>Now we have learnt regression modeling of binomial or multinomial responses. How about count respones, nonnegative real responses, and so on? <strong>Generalized linear model (GLM)</strong> is a generic framework than encompasses normal regression, binomial regression, multinomial regression, and others. There are two essential components of the GLM framework: a <em>distribution</em> for response <span>\(Y\)</span> and a <em>link</em> function that relates mean of <span>\(Y\)</span> to covariates <span>\(x\)</span>.</p>
<div>
<h2>Exponential family distribution</h2>
<p>In GLM, the distribution of <span>\(Y\)</span> is from the exponential familty of distributions of form <span>\[
  f(y \mid \theta, \phi) = \exp \left[ \frac{y \theta - b(\theta)}{a(\phi)} + c(y, \phi) \right].
\]</span> <span>\(\theta\)</span> is called the <strong>canonical parameter</strong> and represents the location while <span>\(\phi\)</span> is the <strong>dispersion parameter</strong> and represents the scale. Note the canonical parameter <span>\(\theta\)</span> is not necessarily the mean <span>\(\mu\)</span>.</p>
<ol style="list-style-type:decimal">
<li>Normal or Gaussian: <span>\[
  f(y \mid \theta, \phi) = \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ - \frac{(y - \mu)^2}{2\sigma^2} \right] = \exp \left[ \frac{y\mu - \mu^2/2}{\sigma^2} - \frac 12 \left( \frac{y^2}{\sigma^2} + \log(2\pi \sigma^2) \right) \right].
\]</span> So we can write
<span>\[\begin{eqnarray*}
  \theta &amp;=&amp; \mu \\
  \phi &amp;=&amp; \sigma^2 \\
  a(\phi) &amp;=&amp; \phi \\
  b(\theta) &amp;=&amp; \theta^2/2 \\
  c(y, \phi) &amp;=&amp; -\frac 12 (y^2/\phi + \log(2\pi \phi)).
\end{eqnarray*}\]</span></li>
<li>Binomial:
<span>\[\begin{eqnarray*}
  &amp; &amp; f(y \mid \theta, \phi) = \binom{n}{y} p^y (1 -p)^{n-y} \\
  &amp;=&amp; \exp \left[ y \log p + (n - y) \log (1 - p) + \log \binom{n}{y} \right] \\
  &amp;=&amp; \exp \left[ y \log \frac{p}{1 - p} + n \log (1 - p) + \log \binom{n}{y} \right].
\end{eqnarray*}\]</span>
So we see
<span>\[\begin{eqnarray*}
  \theta &amp;=&amp; \log \frac{p}{1 - p} \\
  \phi &amp;=&amp; 1 \\
  a(\phi) &amp;=&amp; 1 \\
  b(\theta) &amp;=&amp; - n \log (1 - p) = n \log (1 + \exp \theta) \\
  c(y, \phi) &amp;=&amp; \log \binom{n}{y}.
\end{eqnarray*}\]</span></li>
<li>Poisson: <span>\[
  f(y \mid \theta, \phi) = e^{-\mu} \frac{\mu^y}{y!} = \exp (y \log \mu - \mu - \log y!).
\]</span> So we have
<span>\[\begin{eqnarray*}
  \theta &amp;=&amp; \log \mu \\
  \phi &amp;=&amp; 1 \\
  a(\phi) &amp;=&amp; 1 \\
  b(\theta) &amp;=&amp; \exp \theta \\
  c(y, \phi) &amp;=&amp; - \log y!.
\end{eqnarray*}\]</span></li>
<li>Gamma has density <span>\[
  f(y \mid \nu, \lambda) = \frac{1}{\Gamma(\nu)} \lambda^{\nu} y^{\nu - 1} e^{-\lambda y}, \quad y &gt; 0,
\]</span> where <span>\(\nu\)</span> is the shape parameter and <span>\(\lambda\)</span> is the scale parameter. For the purpose of GLM, it’s convenient to reparameterize by <span>\(\lambda = \nu / \mu\)</span> to get <span>\[
  f(y) = \frac{1}{\Gamma(\nu)} \left( \frac{\nu}{\mu} \right)^{\nu} y^{\nu - 1} e^{-y\nu / \mu} = \exp \left\{ \frac{- y \mu^{-1} - \log \mu}{\nu^{-1}} + (\nu-1) \log y + \nu \log \nu - \log \Gamma(\nu) \right\}.
\]</span> Now <span>\(\mathbb{E}Y = \mu\)</span> and <span>\(\mathbb{Var}(Y) = \mu^2 / \nu = (\mathbb{E})^2 / \nu\)</span>. So we have
<span>\[\begin{eqnarray*}
  \theta &amp;=&amp; - \mu^{-1} \\
  \phi &amp;=&amp; \nu^{-1} \\
  a(\phi) &amp;=&amp; \phi \\
  b(\theta) &amp;=&amp; - \log (- \theta) \\
  c(y, \phi) &amp;=&amp; (\phi^{-1} - 1) \log y - \phi^{-1} \log (\phi) - \log \Gamma(\phi^{-1}).
\end{eqnarray*}\]</span>
<p>Some book remove the minus sign in the canonical parameter/link which is fine provided we take account of this in any derivations. For the canonical link <span>\(\eta = \mu^{-1}\)</span>, the systematic component can only be non-negative, which could cause problems. Other possible link is log link <span>\(\eta = \log \mu\)</span> and identity link <span>\(\eta = \mu\)</span>.</p></li>
<li><p>Many other distributions.</p></li>
</ol>
Exponential family distributions have mean and variance
<span>\[\begin{eqnarray*}
  \mathbb{E}Y &amp;=&amp; \mu = b'(\theta) \\
  \mathbb{Var}Y &amp;=&amp; b''(\theta) a(\phi).
\end{eqnarray*}\]</span>
<p><strong>Show this</strong>. Thus the function <span>\(b\)</span> determines the moments of <span>\(Y\)</span>.</p>
</div>
<div>
<h2>Link function</h2>
<p>Given the <em>linear predictor</em> or <em>systematic component</em> <span>\[
  \eta = \beta_0 + x_1 \beta_1 + \cdots + x_p \beta_p = x^T \beta.
\]</span> The <em>link function</em>, <span>\(g\)</span>, relates the mean <span>\(\mathbb{E}Y = \mu\)</span>, to the covariates <span>\[
  \eta = g(\mu).
\]</span> In principal, any monotone continuous and differentiable function will do. But there are some convenient and common choices for the standard GLMs.</p>
<ul>
<li>For Gaussian linear model, the identity link, <span>\(\eta = \mu\)</span>, is the obvious choice.<br>
</li>
<li>For binomial model, we saw logit, probit and cloglog.<br>
</li>
<li>For Poisson model, a standard choice is <span>\(\eta = \log \mu\)</span>.</li>
</ul>
<p>The <em>canonical link</em> has <span>\(g\)</span> such that <span>\(\eta = g(\mu) = \theta\)</span>, the canonical parameter of the exponential family distribution. This means that <span>\(g(b'(\theta))=\theta\)</span>. If a canonical link is used, <span>\(x^T y\)</span> is sufficient for <span>\(\beta\)</span>. The canonical link is mathematically and computationally convenient and is often the natural choice of link.</p>
<table>
<thead>
<tr>
<th>Family</th>
<th>Canonical Link</th>
<th>Variance Function</th>
</tr>
</thead>
<tbody>
<tr>
<td>Normal</td>
<td><span>\(\eta=\mu\)</span></td>
<td>1</td>
</tr>
<tr>
<td>Poisson</td>
<td><span>\(\eta=\log \mu\)</span></td>
<td><span>\(\mu\)</span></td>
</tr>
<tr>
<td>Binomial</td>
<td><span>\(\eta=\log \left( \frac{\mu}{1 - \mu} \right)\)</span></td>
<td><span>\(\mu (1 - \mu)\)</span></td>
</tr>
<tr>
<td>Gamma</td>
<td><span>\(\eta = \mu^{-1}\)</span></td>
<td><span>\(\mu^2\)</span></td>
</tr>
<tr>
<td>Inverse Gaussian</td>
<td><span>\(\eta = \mu^{-2}\)</span></td>
<td><span>\(\mu^3\)</span></td>
</tr>
</tbody>
</table>
</div>
<div>
<h2>Fisher scoring algorithm and IRWLS</h2>
GLM regreesion coefficients are estimated by MLE. Recall that the Newton-Raphson algorithm for maximizing a log-likelihood <span>\(L(\beta)\)</span> proceeds as <span>\[
  \beta^{(t+1)} = \beta^{(t)} + s [- d^2 L(\beta^{(t)})]^{-1} \nabla L(\beta^{(t)}),
\]</span> where <span>\(s&gt;0\)</span> is a step length, <span>\(\nabla L\)</span> is the score (gradient) vector, and <span>\(-d^2L\)</span> is the observed information matrix (negative Hessian). For GLM,
<span>\[\begin{eqnarray*}
  L(\beta) &amp;=&amp; \sum_{i=1}^n \frac{y_i \theta - b(\theta)}{a(\phi)} + c(y_i, \phi) \\
  \nabla L(\beta) &amp;=&amp; \sum_{i=1}^n \frac{(y_i - \mu_i) \mu_i'(\eta_i)}{\sigma_i^2} x_i \\
  - d^2L(\beta) &amp;=&amp; \sum_{i=1}^n \frac{[\mu_i'(\eta_i)]^2}{\<wbr>sigma_i^2} x_i x_i^T - \sum_{i=1}^n \frac{(y_i - \mu_i) \theta''(\eta_i)}{\sigma_i^2} x_i x_i^T
\end{eqnarray*}\]</span>
<p><strong>Show this</strong>. For GLMs with canonical links, we have <span>\(\theta''(\eta_i)=0\)</span> thus the second term in negative Hessian vanishes. It’s immediate that the negative Hessian is positive semidefinte and Newton’s algorithm with line search is stable. How about non-canonical link? We use the expected (Fisher) information matrix <span>\[
  \mathbb{E} [- d^2L(\beta)] = \sum_{i=1}^n \frac{[\mu_i'(\eta_i)]^2}{\<wbr>sigma_i^2} x_i x_i^T = X^T W X \succeq 0,
\]</span> where <span>\(W = \text{diag}([\mu_i'(\eta_i)]^<wbr>2/\sigma_i^2)\)</span>.</p>
Take the logistic regression as an example
<span>\[\begin{eqnarray*}
  L(\beta) &amp;=&amp; \sum_{i=1}^n [y_i \log p_i + (1 - y_i) \log (1 - p_i)] = \sum_{i=1}^n [y_i x_i^T \beta - \log (1 + e^{x_i^T \beta})] \\
  \nabla L(\beta) &amp;=&amp; \sum_{i=1}^n \left( y_i x_i - \frac{\exp x_i^T \beta}{1 + \exp x_i^T \beta} x_i \right) = \sum_{i=1}^n (y_i - p_i) x_i = X^T (y - p) \\
  - d^2 L(\beta) &amp;=&amp; \sum_{i=1}^n p_i (1 - p_i) x_i x_i^T = X^T W X, \quad W = \text{diag}(w_1, \ldots, w_n), w_i = p_i (1 - p_i) \\
  \mathbb{E} [- d^2 L(\beta)] &amp;=&amp; - d^2 L(\beta).
\end{eqnarray*}\]</span>
The Fisher scoring algorithmn proceeds
<span>\[\begin{eqnarray*}
  \beta^{(t+1)} &amp;=&amp; \beta^{(t)} + s(X^T W^{(t)} X)^{-1} X^T (y - p^{(t)}) \\
  &amp;=&amp; (X^T W^{(t)} X)^{-1} X^T W^{(t)} [X \beta^{(t)} + s (W^{(t)})^{-1} (y - p^{(t)})] \\
  &amp;=&amp; (X^T W^{(t)} X)^{-1} X^T W^{(t)} z^{(t)},
\end{eqnarray*}\]</span>
<p>where <span>\[
  z^{(t)} = X \beta^{(t)} + s (W^{(t)})^{-1} (y - p^{(t)})
\]</span> are <em>working responses</em>. In this sense, the Fisher scoring algorithm for GLM is also called the IRWLS (ieteratively reweighted least squares).</p>
</div>
<div>
<h2>Hypothesis testing</h2>
<p>When considering the choice of model for some data, two extremes are the <em>null</em> model and the <em>full</em> or <em>saturated</em> model.</p>
<ul>
<li>The null model means there’s no relation between predictors and the response. Usually it means we fit a common mean <span>\(\mu\)</span> for all <span>\(y\)</span>.<br>
</li>
<li>The full model means data is explaine exactly. Typically it means we need to use <span>\(n\)</span> parameters for <span>\(n\)</span> data points.</li>
</ul>
<p>To assess the <em>goodness of fit</em> of a model, we might consider likelihood ratio statistic: <span>\[
  2 [L(y, \phi \mid y) - L(\hat \mu, \phi \mid y)]
\]</span> For independent observations from exponential family with <span>\(a_i(\phi) = \phi\)</span>, this simplifies to <span>\[
  \frac{D(y, \hat \mu)}{\phi} = \frac{2 \sum_i [y_i(\tilde \theta_i - \hat \theta_i) - b(\tilde \theta_i) + b(\hat \theta_i)]}{\phi},
\]</span> where <span>\(\tilde \theta\)</span> are the estimates under the full model and <span>\(\hat \theta\)</span> are the estimates under the model of interest. <span>\(D(y, \hat \mu)\)</span> is called the <strong>deviance</strong> and <span>\(D(y, \hat \mu) / \phi\)</span> is the <strong>scaled deviance</strong>.</p>
<p>An alternative measure of goodness of fit is the <em>Pearson’s <span>\(X^2\)</span> statistic</em> <span>\[
  X^2 = \sum_i \frac{(y_i - \hat \mu_i)^2}{\mathbb{Var}(\hat \mu)}.
\]</span></p>
<table>
<colgroup>
<col width="17%">
<col width="82%">
</colgroup>
<thead>
<tr>
<th>GLM</th>
<th>Deviance</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gaussian</td>
<td><span>\(\sum_i (y_i - \hat \mu_i)^2\)</span></td>
</tr>
<tr>
<td>Poisson</td>
<td><span>\(2\sum_i [y_i \log(y_i / \hat \mu_i) - (y_i - \hat \mu_i)]\)</span></td>
</tr>
<tr>
<td>Binomial</td>
<td><span>\(2 \sum_i [y_i \log(y_i / \hat \mu_i) + (n_i - y_i) \log((n_i - y_i)/(n_i - \hat \mu_i))]\)</span></td>
</tr>
<tr>
<td>Gamma</td>
<td><span>\(2 \sum_i [- \log(y_i / \hat \mu_i) + (y_i - \hat \mu_i) / \hat \mu_i]\)</span></td>
</tr>
<tr>
<td>Inverse Gaussian</td>
<td><span>\(\sum_i (y_i - \hat \mu_i)^2 / (\hat \mu_i^2 y_i)\)</span></td>
</tr>
</tbody>
</table>
<p>For goodness of fit test, we use the fact that, under certain conditions, provided the model is correct, the scalled Deviance and the Pearson’s <span>\(X^2\)</span> statistic are both asymptotically <span>\(\chi^2\)</span> with degrees of freedom equal to the number of identifiable parameters.</p>
<p>For Gaussian, <span>\(\phi\)</span> is unknown so this test cannot be used. For binomial and Poisson, <span>\(\phi=1\)</span> so the test is practical. However the accuracy of asymptotic approximation is dubious for smaller data sets. For binary responses, the approximation is worthless.</p>
<p>To compare two nested models <span>\(\Omega\)</span> and <span>\(\omega\)</span>, difference of the scaled deviance <span>\(D_\omega - D_\Omega\)</span> is asymptotically <span>\(\chi^2\)</span> with degrees of freedom equal to the difference in the number of identifiable parameters in the two models. For Gaussian model and other models where the disperson <span>\(\phi\)</span> is unknown, we can insert an estimate of <span>\(\phi\)</span> and compute an <span>\(F\)</span> test <span>\[
  \frac{(D_\omega - D_\Omega) / (\text{df}_{\omega} - \text{df}_{\Omega})}{\hat \phi},
\]</span> where <span>\(\hat \phi = X^2 / (n-p)\)</span> is a good estimate of the dispersion. For Gaussian, the F-test is exact. For other models, the F-test is approximate.</p>
</div>
<div>
<h2>Diagnostics</h2>
<div>
<h3>Residuals</h3>
<p><em>Pearson residual</em> <span>\[
  r_p = \frac{y - \hat \mu}{\sqrt{\mathbb{Var}(\hat \mu)}}.
\]</span> <em>Deviance residual</em> <span>\[
  r_D = \text{sign}(y - \hat \mu) \sqrt{d_i},
\]</span> where <span>\(d_i\)</span> are summands in the calculation of deviance.</p>
<pre><code>data(bliss)
modl &lt;- glm(cbind(dead, alive) ~ conc, family = binomial, data = bliss)
residuals(modl) # deviance residuals</code></pre>
<pre><code>##           1           2           3           4           5 
## -0.45101510  0.35969607  0.00000000  0.06430235 -0.20449347</code></pre>
<pre><code>residuals(modl, "pearson") # Pearson residuals</code></pre>
<pre><code>##             1             2             3             4             5 
## -4.325234e-01  3.643729e-01 -3.648565e-15  6.414687e-02 -2.081068e-01</code></pre>
<pre><code>residuals(modl, "response") # response - fitted values</code></pre>
<pre><code>##             1             2             3             4             5 
## -2.250510e-02  2.834353e-02 -3.330669e-16  4.989802e-03 -1.082823e-02</code></pre>
<pre><code>residuals(modl, "working") # working response</code></pre>
<pre><code>##             1             2             3             4             5 
## -2.770876e-01  1.561410e-01 -1.332268e-15  2.748820e-02 -1.333195e-01</code></pre>
<pre><code>modl$residuals # same as working residuals</code></pre>
<pre><code>##             1             2             3             4             5 
## -2.770876e-01  1.561410e-01 -1.332268e-15  2.748820e-02 -1.333195e-01</code></pre>
<p>We mostly use the deviance residuals for diagnostics.</p>
</div>
<div>
<h3>Leverage and influence</h3>
<p>For GLM, the hat matrix is <span>\[
  H = W^{1/2}X (X^T W X)^{-1} X^T W^{1/2},
\]</span> where <span>\(W\)</span> is the weight matrix at the fitted model. Diagonal elements of <span>\(H\)</span> are the leverages <span>\(h_i\)</span>. A larger value of leverage indicates that the fit may be sensitive to the response at case <span>\(i\)</span>. Its predictor values are unusual in some way.</p>
<pre><code>influence(modl)$hat</code></pre>
<pre><code>##         1         2         3         4         5 
## 0.4255049 0.4133068 0.3223765 0.4133068 0.4255049</code></pre>
<p>The studentized residuals are <span>\[
  r_{SD} = \frac{r_D}{\sqrt{\hat \phi (1 - h_i)}}.
\]</span></p>
<pre><code>rstudent(modl)</code></pre>
<pre><code>##           1           2           3           4           5 
## -0.58478586  0.47213544  0.00000000  0.08386629 -0.27183519</code></pre>
<p>Leverage only measures the potential to affect the fit whereas measures of influecen more directly access the effect of each case on the fit.</p>
<pre><code>influence(modl)$coef</code></pre>
<pre><code>##    (Intercept)         conc
## 1 -0.214001479  0.080663550
## 2  0.155671882 -0.047087302
## 3  0.000000000  0.000000000
## 4 -0.005841678  0.008417729
## 5  0.049263918 -0.036573429</code></pre>
<p>Alternatively we can examine the Cook statistics <span>\[
  D_i = \frac{(\hat \beta_{(i)} - \hat \beta)^T (X^T W X) (\hat \beta_{(i)} - \hat \beta)}{p \hat \phi}.
\]</span></p>
<pre><code>cooks.distance(modl)</code></pre>
<pre><code>##            1            2            3            4            5 
## 1.205927e-01 7.970999e-02 4.673053e-30 2.470424e-03 2.791738e-02</code></pre>
</div>
<div>
<h3>Residual plots</h3>
<p>For GLM, it’s better to plot the linear predictors <span>\(\hat \eta\)</span> rather than the predicted responses. We take a look at a Poisson regression example on the Galapagos data.</p>
<pre><code>data(gala)
gala</code></pre>
<pre><code>##              Species Endemics    Area Elevation Nearest Scruz Adjacent
## Baltra            58       23   25.09       346     0.6   0.6     1.84
## Bartolome         31       21    1.24       109     0.6  26.3   572.33
## Caldwell           3        3    0.21       114     2.8  58.7     0.78
## Champion          25        9    0.10        46     1.9  47.4     0.18
## Coamano            2        1    0.05        77     1.9   1.9   903.82
## Daphne.Major      18       11    0.34       119     8.0   8.0     1.84
## Daphne.Minor      24        0    0.08        93     6.0  12.0     0.34
## Darwin            10        7    2.33       168    34.1 290.2     2.85
## Eden               8        4    0.03        71     0.4   0.4    17.95
## Enderby            2        2    0.18       112     2.6  50.2     0.10
## Espanola          97       26   58.27       198     1.1  88.3     0.57
## Fernandina        93       35  634.49      1494     4.3  95.3  4669.32
## Gardner1          58       17    0.57        49     1.1  93.1    58.27
## Gardner2           5        4    0.78       227     4.6  62.2     0.21
## Genovesa          40       19   17.35        76    47.4  92.2   129.49
## Isabela          347       89 4669.32      1707     0.7  28.1   634.49
## Marchena          51       23  129.49       343    29.1  85.9    59.56
## Onslow             2        2    0.01        25     3.3  45.9     0.10
## Pinta            104       37   59.56       777    29.1 119.6   129.49
## Pinzon           108       33   17.95       458    10.7  10.7     0.03
## Las.Plazas        12        9    0.23        94     0.5   0.6    25.09
## Rabida            70       30    4.89       367     4.4  24.4   572.33
## SanCristobal     280       65  551.62       716    45.2  66.6     0.57
## SanSalvador      237       81  572.33       906     0.2  19.8     4.89
## SantaCruz        444       95  903.82       864     0.6   0.0     0.52
## SantaFe           62       28   24.08       259    16.5  16.5     0.52
## SantaMaria       285       73  170.92       640     2.6  49.2     0.10
## Seymour           44       16    1.84       147     0.6   9.6    25.09
## Tortuga           16        8    1.24       186     6.8  50.9    17.95
## Wolf              21       12    2.85       253    34.1 254.7     2.33</code></pre>
<pre><code>gala &lt;- gala[, -2]
modp &lt;- glm(Species ~ ., family = poisson, gala)
plot(residuals(modp) ~ predict(modp, type = "response"), xlab = expression(hat(mu)), ylab = "Deviance residuals")</code></pre>
<p><img width="672"></p>
<pre><code>plot(residuals(modp) ~ predict(modp, type = "link"), xlab = expression(hat(mu)), ylab = "Deviance residuals")</code></pre>
<p><img width="672"></p>
<pre><code>plot(residuals(modp, type = "response") ~ predict(modp, type = "link"), xlab = expression(hat(eta)), ylab = "Response residuals")</code></pre>
<p><img width="672"></p>
<p>Q-Q plot of the residuals is the standard way to check the normality assumption on the errors. For GLM, it’s better use a half-normal plot that compares the sorted absolute residuals and the quantiles of the half-normal distribution <span>\[
  \Phi^{-1} \left( \frac{n+i}{2n + i} \right), \quad i=1,\ldots,n.
\]</span> The residuals are not expected to be normally distributed, so we are not looking for an approximate straight line. We only seek outliers which may be identified as points off the trend. A half-normal plot is better for this purpose because in a sense the resolution of the plot is doubled by having all the points in one tail.</p>
<pre><code>#halfnorm(rstudent(modepl))
#gali &lt;- influence(modpl)
#halfnorm(gali$hat)</code></pre>
</div>
</div>
</div>




</div>






</div>

</body></html>